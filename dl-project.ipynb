{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4849320,"sourceType":"datasetVersion","datasetId":2807884}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"markdown","source":"## Frame Extraction","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nfrom tqdm import tqdm\n\n# Define the path to the dataset\ndataset_path = '/kaggle/input/ucf101-action-recognition/train'\ndataset_path_val = '/kaggle/input/ucf101-action-recognition/val'\n\n\n# Define the path where extracted frames will be saved\nframes_output_path = '/kaggle/working/frames/train'\nframes_output_path_val = '/kaggle/working/frames/val'\n\n# Create the output directory if it doesn't exist\nif not os.path.exists(frames_output_path):\n    os.makedirs(frames_output_path)\n\n# Create the output directory if it doesn't exist\nif not os.path.exists(frames_output_path_val):\n    os.makedirs(frames_output_path_val)\n\n# List of selected classes\nclasses = ['PushUps', 'PullUps', 'BenchPress', 'Lunges', 'WallPushups']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_frames_from_train_videos():\n    for class_name in classes:\n        print(f\"Processing class: {class_name}\")\n        class_video_path = os.path.join(dataset_path, class_name)\n        class_frames_output_path = os.path.join(frames_output_path, class_name)\n        \n        if not os.path.exists(class_frames_output_path):\n            os.makedirs(class_frames_output_path)\n        \n        video_files = os.listdir(class_video_path)\n        \n        for video_file in tqdm(video_files):\n            video_path = os.path.join(class_video_path, video_file)\n            video_filename = os.path.splitext(video_file)[0]\n            video_capture = cv2.VideoCapture(video_path)\n            \n            frame_count = 0\n            success = True\n            while success:\n                success, frame = video_capture.read()\n                if success:\n                    # Resize to 64x64 pixels\n                    frame = cv2.resize(frame, (64, 64), interpolation=cv2.INTER_AREA)\n                    # Convert to grayscale\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n                    # Save the frame as an image\n                    frame_filename = f\"{video_filename}_frame_{frame_count:05d}.jpg\"\n                    frame_filepath = os.path.join(class_frames_output_path, frame_filename)\n                    cv2.imwrite(frame_filepath, frame)\n                    frame_count += 1\n            video_capture.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_frames_from_val_videos():\n    for class_name in classes:\n        print(f\"Processing class: {class_name}\")\n        class_video_path = os.path.join(dataset_path_val, class_name)\n        class_frames_output_path = os.path.join(frames_output_path_val, class_name)\n        \n        if not os.path.exists(class_frames_output_path):\n            os.makedirs(class_frames_output_path)\n        \n        video_files = os.listdir(class_video_path)\n        \n        for video_file in tqdm(video_files):\n            video_path = os.path.join(class_video_path, video_file)\n            video_filename = os.path.splitext(video_file)[0]\n            video_capture = cv2.VideoCapture(video_path)\n            \n            frame_count = 0\n            success = True\n            while success:\n                success, frame = video_capture.read()\n                if success:\n                    # Resize to 64x64 pixels\n                    frame = cv2.resize(frame, (64, 64), interpolation=cv2.INTER_AREA)\n                    # Convert to grayscale\n                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n                    # Save the frame as an image\n                    frame_filename = f\"{video_filename}_frame_{frame_count:05d}.jpg\"\n                    frame_filepath = os.path.join(class_frames_output_path, frame_filename)\n                    cv2.imwrite(frame_filepath, frame)\n                    frame_count += 1\n            video_capture.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"extract_frames_from_train_videos()\nextract_frames_from_val_videos()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Video List","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\n\ntrain_csv_path = '/kaggle/input/ucf101-action-recognition/train.csv'\nval_csv_path = '/kaggle/input/ucf101-action-recognition/val.csv'\n\ntrain_df = pd.read_csv(train_csv_path)\nval_df = pd.read_csv(val_csv_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_video_list(df, classes):\n    # Filter the DataFrame for selected classes\n    filtered_df = df[df['label'].isin(classes)]\n    \n    # Extract class names and video filenames\n    video_list = list(zip(filtered_df['label'], filtered_df['clip_path'].apply(lambda x: os.path.basename(x))))\n    \n    return video_list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate the lists\ntrain_videos = create_video_list(train_df, classes)\nval_videos = create_video_list(val_df, classes)\n\n# Optional: Print the number of videos in each list\nprint(f\"Number of training videos: {len(train_videos)}\")\nprint(f\"Number of validation videos: {len(val_videos)}\")\n\n# Optional: Print the first few entries to verify\nprint(\"First 5 training videos:\")\nfor video in train_videos[:5]:\n    print(video)\n\nprint(\"\\nFirst 5 validation videos:\")\nfor video in val_videos[:5]:\n    print(video)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Generation","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.utils import Sequence\nimport cv2\nimport os\nimport random\n\nclass FrameSequenceGenerator(Sequence):\n    def __init__(self, video_list, batch_size, input_length, target_length, frames_path, augment=False):\n        self.video_list = video_list\n        self.batch_size = batch_size\n        self.input_length = input_length\n        self.target_length = target_length\n        self.frames_path = frames_path\n        self.augment = augment\n\n        # Precompute all possible sequences\n        self.sequences = []\n        for class_name, video_filename in self.video_list:\n            frames_folder = os.path.join(self.frames_path, class_name)\n            frame_prefix = os.path.splitext(video_filename)[0]\n            frame_files = sorted([\n                f for f in os.listdir(frames_folder)\n                if f.startswith(frame_prefix) and f.endswith('.jpg')\n            ])\n\n            total_required = self.input_length + self.target_length\n            if len(frame_files) < total_required:\n                continue  # Skip if not enough frames\n\n            for i in range(len(frame_files) - total_required + 1):\n                input_frame_paths = [\n                    os.path.join(frames_folder, frame_files[j])\n                    for j in range(i, i + self.input_length)\n                ]\n                target_frame_paths = [\n                    os.path.join(frames_folder, frame_files[j])\n                    for j in range(i + self.input_length, i + total_required)\n                ]\n\n                self.sequences.append((input_frame_paths, target_frame_paths))\n\n    def __len__(self):\n        # Number of batches per epoch\n        return int(np.ceil(len(self.sequences) / self.batch_size))\n\n    def __getitem__(self, idx):\n        # Generate one batch of data\n        batch_sequences = self.sequences[idx * self.batch_size:(idx + 1) * self.batch_size]\n        X_batch = []\n        y_batch = []\n\n        for input_frame_paths, target_frame_paths in batch_sequences:\n            # Load and preprocess input frames\n            input_sequence = [self.load_and_preprocess_frame(fp, augment=self.augment) for fp in input_frame_paths]\n            # Load and preprocess target frames\n            target_sequence = [self.load_and_preprocess_frame(fp, augment=False) for fp in target_frame_paths]\n\n            X_batch.append(np.array(input_sequence))\n            y_batch.append(np.array(target_sequence))\n\n        # Convert lists to numpy arrays\n        X_batch = np.array(X_batch)\n        y_batch = np.array(y_batch)\n\n        return X_batch, y_batch\n\n    @staticmethod\n    def load_and_preprocess_frame(frame_path, augment=False):\n        # Load the image in grayscale mode\n        frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)\n        if frame is None:\n            raise ValueError(f\"Failed to load image at {frame_path}\")\n\n        # Apply random horizontal flip\n        if augment and random.random() < 0.5:\n            frame = cv2.flip(frame, 1)\n\n        # Normalize pixel values to [0, 1]\n        frame = frame / 255.0\n\n        # Expand dimensions to add the channel dimension\n        frame = np.expand_dims(frame, axis=-1)\n\n        return frame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parameters\nbatch_size = 16\ninput_length = 20\ntarget_length = 10\n\n# Paths to your extracted frames\ntrain_frames_path = '/kaggle/working/frames/train'\nval_frames_path = '/kaggle/working/frames/val'\n\n# Instantiate the data generators with augmentation for training\ntrain_generator = FrameSequenceGenerator(\n    video_list=train_videos,\n    batch_size=batch_size,\n    input_length=input_length,\n    target_length=target_length,\n    frames_path=train_frames_path,\n    augment=True\n)\n\n# Validation generator without augmentation\nval_generator = FrameSequenceGenerator(\n    video_list=val_videos,\n    batch_size=batch_size,\n    input_length=input_length,\n    target_length=target_length,\n    frames_path=val_frames_path,\n    augment=False\n)\nprint(f\"Number of training batches: {len(train_generator)}\")\nprint(f\"Number of validation batches: {len(val_generator)}\")\n\n# Fetch a batch from the training generator\nX_batch, y_batch = train_generator.__getitem__(0)\n\n# Print the shapes\nprint(f\"Input batch shape: {X_batch.shape}\")    # Expected: (batch_size, input_length, 64, 64, 1)\nprint(f\"Target batch shape: {y_batch.shape}\")  # Expected: (batch_size, target_length, 64, 64, 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualize_sequence(input_seq, target_seq):\n    num_input = input_seq.shape[0]\n    num_target = target_seq.shape[0]\n\n    fig, axes = plt.subplots(3, max(num_input, num_target), figsize=(30, 4))\n\n    # Plot input frames\n    for i in range(num_input):\n        axes[0, i].imshow(input_seq[i].squeeze(), cmap='gray')\n        axes[0, i].axis('off')\n        axes[0, i].set_title(f\"Input Frame {i+1}\")\n\n    # Plot target frames\n    for i in range(num_target):\n        axes[1, i].imshow(target_seq[i].squeeze(), cmap='gray')\n        axes[1, i].axis('off')\n        axes[1, i].set_title(f\"Target Frame {i+1}\")\n\n    plt.show()\n\n# Visualize the first sequence in the batch\nvisualize_sequence(X_batch[0], y_batch[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Implementation","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport os\nimport cv2\nfrom skimage.metrics import structural_similarity as ssim\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VideoFrameDataset(Dataset):\n    def __init__(self, video_list, frames_path, input_length=10, target_length=10, augment=False):\n        self.video_list = video_list\n        self.frames_path = frames_path\n        self.input_length = input_length\n        self.target_length = target_length\n        self.augment = augment\n        self.sequences = []\n\n        for class_name, video_filename in self.video_list:\n            frames_folder = os.path.join(self.frames_path, class_name)\n            frame_prefix = os.path.splitext(video_filename)[0]\n            frame_files = sorted([\n                f for f in os.listdir(frames_folder)\n                if f.startswith(frame_prefix) and f.endswith('.jpg')\n            ])\n            total_required = self.input_length + self.target_length\n            if len(frame_files) < total_required:\n                continue\n            for i in range(len(frame_files) - total_required + 1):\n                input_frames = frame_files[i:i + self.input_length]\n                target_frames = frame_files[i + self.input_length:i + total_required]\n                self.sequences.append((frames_folder, input_frames, target_frames))\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        frames_folder, input_frames, target_frames = self.sequences[idx]\n        input_sequence = [self.load_frame(os.path.join(frames_folder, f)) for f in input_frames]\n        target_sequence = [self.load_frame(os.path.join(frames_folder, f)) for f in target_frames]\n\n        input_seq = np.stack(input_sequence, axis=0)  # [t, 1, H, W]\n        target_seq = np.stack(target_sequence, axis=0)  # [t, 1, H, W]\n        \n        input_seq = torch.tensor(input_seq, dtype=torch.float32)\n        target_seq = torch.tensor(target_seq, dtype=torch.float32)\n        \n        return input_seq, target_seq\n\n    @staticmethod\n    def load_frame(frame_path):\n        frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)\n        frame = frame / 255.0  # Normalize\n        frame = np.expand_dims(frame, axis=0)  # [1, H, W]\n        return frame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Instantiate the dataset\ntrain_dataset = VideoFrameDataset(\n    video_list=train_videos,\n    frames_path=train_frames_path,\n    input_length=20,\n    target_length=10,\n    augment=True\n)\n\nval_dataset = VideoFrameDataset(\n    video_list=val_videos,\n    frames_path=val_frames_path,\n    input_length=20,\n    target_length=10,\n    augment=False\n)\n\n# Instantiate dataloaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ConvLSTM","metadata":{}},{"cell_type":"code","source":"class ConvLSTMCell(nn.Module):\n    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n        super(ConvLSTMCell, self).__init__()\n\n        padding = kernel_size[0] // 2, kernel_size[1] // 2\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n                              out_channels=4 * self.hidden_dim,\n                              kernel_size=kernel_size,\n                              padding=padding,\n                              bias=bias)\n\n    def forward(self, input_tensor, cur_state):\n        h_cur, c_cur = cur_state\n\n        # Concatenate along channel axis\n        combined = torch.cat([input_tensor, h_cur], dim=1)  # (batch, input_dim + hidden_dim, height, width)\n        combined_conv = self.conv(combined)\n        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n        i = torch.sigmoid(cc_i)\n        f = torch.sigmoid(cc_f)\n        o = torch.sigmoid(cc_o)\n        g = torch.tanh(cc_g)\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n\n        return h_next, c_next\n\n    def init_hidden(self, batch_size, image_size):\n        height, width = image_size\n        device = next(self.parameters()).device\n        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=device),\n                torch.zeros(batch_size, self.hidden_dim, height, width, device=device))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvLSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers, batch_first=False, bias=True, return_all_layers=False):\n        super(ConvLSTM, self).__init__()\n\n        self._check_kernel_size_consistency(kernel_size)\n\n        # Make lists\n        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n        hidden_dim  = self._extend_for_multilayer(hidden_dim, num_layers)\n        if not len(kernel_size) == len(hidden_dim) == num_layers:\n            raise ValueError(\"Inconsistent list length.\")\n\n        self.input_dim    = input_dim\n        self.hidden_dim   = hidden_dim\n        self.kernel_size  = kernel_size\n        self.num_layers   = num_layers\n        self.batch_first  = batch_first\n        self.bias         = bias\n        self.return_all_layers = return_all_layers\n\n        cell_list = []\n        for i in range(0, self.num_layers):\n            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i-1]\n\n            cell = ConvLSTMCell(input_dim=cur_input_dim,\n                                hidden_dim=self.hidden_dim[i],\n                                kernel_size=self.kernel_size[i],\n                                bias=self.bias)\n\n            cell_list.append(cell)\n\n        self.cell_list = nn.ModuleList(cell_list)\n\n    def forward(self, input_tensor, hidden_state=None):\n        if not self.batch_first:\n            # (t, b, c, h, w) -> (b, t, c, h, w)\n            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n\n        b, _, _, h, w = input_tensor.size()\n\n        # Implement stateful ConvLSTM\n        if hidden_state is None:\n            hidden_state = self._init_hidden(batch_size=b,\n                                             image_size=(h, w))\n\n        layer_output_list = []\n        last_state_list   = []\n\n        seq_len = input_tensor.size(1)\n\n        current_input = input_tensor\n\n        for layer_idx in range(self.num_layers):\n\n            h, c = hidden_state[layer_idx]\n            output_inner = []\n\n            for t in range(seq_len):\n                h, c = self.cell_list[layer_idx](input_tensor=current_input[:, t, :, :, :],\n                                                 cur_state=[h, c])\n                output_inner.append(h)\n\n            layer_output = torch.stack(output_inner, dim=1)\n            current_input = layer_output\n\n            layer_output_list.append(layer_output)\n            last_state_list.append([h, c])\n\n        if not self.return_all_layers:\n            layer_output_list = layer_output_list[-1:]\n            last_state_list   = last_state_list[-1:]\n\n        return layer_output_list, last_state_list\n\n    def _init_hidden(self, batch_size, image_size):\n        init_states = []\n        for i in range(self.num_layers):\n            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n        return init_states\n\n    @staticmethod\n    def _check_kernel_size_consistency(kernel_size):\n        if not (isinstance(kernel_size, list) or isinstance(kernel_size, tuple)):\n            raise ValueError('`kernel_size` must be a list or tuple')\n        for ks in kernel_size:\n            if not (isinstance(ks, list) or isinstance(ks, tuple)):\n                raise ValueError('`kernel_size` must be a list or tuple of list or tuples')\n    \n    @staticmethod\n    def _extend_for_multilayer(param, num_layers):\n        if isinstance(param, list) or isinstance(param, tuple):\n            return param\n        return [param] * num_layers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ConvLSTMNet(nn.Module):\n    def __init__(self, input_length=30, target_length=15, img_height=64, img_width=64, channels=1, hidden_dims=[32, 32]):\n        super(ConvLSTMNet, self).__init__()\n        self.input_length = input_length\n        self.target_length = target_length\n        self.img_height = img_height\n        self.img_width = img_width\n        self.channels = channels\n\n        # Define kernel_size as a list of tuples, one per layer\n        kernel_size = [(3, 3) for _ in hidden_dims]\n\n        self.convlstm = ConvLSTM(\n            input_dim=channels,\n            hidden_dim=hidden_dims,\n            kernel_size=kernel_size,\n            num_layers=len(hidden_dims),\n            batch_first=True,\n            bias=True,\n            return_all_layers=False\n        )\n\n        self.conv = nn.Conv3d(\n            in_channels=hidden_dims[-1],\n            out_channels=channels,\n            kernel_size=(3, 3, 3),\n            padding=(1, 1, 1)\n        )\n        self.activation = nn.Sigmoid()\n\n    def forward(self, x):\n        # Pass through ConvLSTM\n        convlstm_out, _ = self.convlstm(x)\n        convlstm_out = convlstm_out[0]\n\n        # Apply Conv3D to generate predictions\n        convlstm_out = convlstm_out.permute(0, 2, 1, 3, 4)\n        pred = self.conv(convlstm_out)\n        pred = self.activation(pred)\n\n        # Permute back to (batch, seq_len, channels, h, w)\n        pred = pred.permute(0, 2, 1, 3, 4)\n\n        # Take the last target_length frames\n        pred = pred[:, -self.target_length:, :, :, :]\n\n        return pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate the ConvLSTMNet\nmodel = ConvLSTMNet()\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training parameters\nnum_epochs = 5\n\n# Initialize lists to store losses and metrics\ntrain_losses = []\nval_losses = []\nval_mse_scores = []\nval_ssim_scores = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        # Move to device\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * X_batch.size(0)\n    \n    train_loss /= len(train_loader.dataset)\n    train_losses.append(train_loss)\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    total_mse = 0.0\n    total_ssim = 0.0\n    num_samples = 0\n    \n    with torch.no_grad():\n        for X_batch, y_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            val_loss += loss.item() * X_batch.size(0)\n            \n            # Compute MSE and SSIM per frame\n            batch_size = X_batch.size(0)\n            num_samples += batch_size\n            outputs_np = outputs.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy()\n            \n            for i in range(batch_size):\n                mse_per_video = []\n                ssim_per_video = []\n                for t in range(outputs_np.shape[1]):\n                    # Predicted and ground truth frames\n                    pred_frame = outputs_np[i, t, 0]  # Assuming single channel\n                    true_frame = y_batch_np[i, t, 0]\n                    \n                    # Compute MSE\n                    mse_frame = np.mean((pred_frame - true_frame) ** 2)\n                    mse_per_video.append(mse_frame)\n                    \n                    # Compute SSIM\n                    ssim_frame = ssim(true_frame, pred_frame, data_range=1.0)\n                    ssim_per_video.append(ssim_frame)\n                \n                # Average over frames in the sequence\n                total_mse += np.mean(mse_per_video)\n                total_ssim += np.mean(ssim_per_video)\n    \n    val_loss /= len(val_loader.dataset)\n    val_losses.append(val_loss)\n    \n    # Average MSE and SSIM over the validation set\n    avg_mse = total_mse / num_samples\n    avg_ssim = total_ssim / num_samples\n    val_mse_scores.append(avg_mse)\n    val_ssim_scores.append(avg_ssim)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}, MSE: {avg_mse:.6f}, SSIM: {avg_ssim:.6f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PredRNN","metadata":{}},{"cell_type":"code","source":"class ST_LSTMCell(nn.Module):\n    def __init__(self, input_dim, hidden_dim, kernel_size=(3, 3)):\n        super(ST_LSTMCell, self).__init__()\n        self.hidden_dim = hidden_dim  # Store the hidden dimension\n        padding = kernel_size[0] // 2\n\n        self.conv_x = nn.Conv2d(input_dim, 4 * hidden_dim, kernel_size, padding=padding)\n        self.conv_h = nn.Conv2d(hidden_dim, 4 * hidden_dim, kernel_size, padding=padding)\n\n    def forward(self, x, h, c):\n        gates_x = self.conv_x(x)\n        gates_h = self.conv_h(h)\n        gates = gates_x + gates_h\n\n        i, f, o, g = torch.split(gates, gates.size(1) // 4, dim=1)\n        i = torch.sigmoid(i)\n        f = torch.sigmoid(f)\n        o = torch.sigmoid(o)\n        g = torch.tanh(g)\n\n        c_next = f * c + i * g\n        h_next = o * torch.tanh(c_next)\n\n        return h_next, c_next\n\nclass PredRNN(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers):\n        super(PredRNN, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n        \n        self.layers = nn.ModuleList(\n            [ST_LSTMCell(input_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)]\n        )\n        # Output convolution to produce the final predicted frame\n        self.output_conv = nn.Conv2d(hidden_dim, input_dim, kernel_size=3, padding=1)\n    \n    def forward(self, input_seq, future_seq_length):\n        batch_size, seq_len, _, height, width = input_seq.size()\n        \n        # Initialize hidden and cell states\n        h = [torch.zeros(batch_size, self.hidden_dim, height, width).to(input_seq.device) for _ in range(self.num_layers)]\n        c = [torch.zeros(batch_size, self.hidden_dim, height, width).to(input_seq.device) for _ in range(self.num_layers)]\n        \n        outputs = []\n        \n        # Encoding phase\n        for t in range(seq_len):\n            x = input_seq[:, t]\n            for i, layer in enumerate(self.layers):\n                h[i], c[i] = layer(x, h[i], c[i])\n                x = h[i]\n            # Optionally, collect outputs during the encoding phase\n            # outputs.append(self.output_conv(h[-1]))\n        \n        # Decoding phase\n        x = input_seq[:, -1]  # Start with the last input frame\n        for t in range(future_seq_length):\n            for i, layer in enumerate(self.layers):\n                h[i], c[i] = layer(x, h[i], c[i])\n                x = h[i]\n            # Generate the output frame\n            output_frame = self.output_conv(h[-1])\n            outputs.append(output_frame)\n            x = output_frame  # Use the output as the next input (closed loop)\n        \n        # Stack outputs and return\n        output_seq = torch.stack(outputs, dim=1)  # [batch_size, future_seq_length, channels, height, width]\n        return output_seq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_loader, optimizer, criterion, device, num_epochs=5, target_length=10):\n    model.train()\n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        for input_seq, target_seq in tqdm(train_loader):\n            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n            \n            optimizer.zero_grad()\n            output_seq = model(input_seq, future_seq_length=target_length)\n            \n            loss = criterion(output_seq, target_seq)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.metrics import structural_similarity as ssim\n\ndef evaluate_model(model, val_loader, device, target_length=10):\n    model.eval()\n    total_mse = 0\n    total_ssim = 0\n    num_batches = len(val_loader)\n    \n    with torch.no_grad():\n        for input_seq, target_seq in tqdm(val_loader):\n            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n            output_seq = model(input_seq, future_seq_length=target_length)\n            \n            mse = ((output_seq - target_seq) ** 2).mean().item()\n            total_mse += mse\n            \n            # Calculate SSIM for each frame in the sequence\n            output_np = output_seq.cpu().numpy()\n            target_np = target_seq.cpu().numpy()\n            \n            batch_ssim = 0\n            for i in range(output_np.shape[0]):  # Iterate over batch\n                for t in range(output_np.shape[1]):  # Iterate over sequence length\n                    ssim_frame = ssim(\n                        target_np[i, t, 0], \n                        output_np[i, t, 0], \n                        data_range=1.0, \n                        win_size=11  # Adjust as needed\n                    )\n                    batch_ssim += ssim_frame\n            total_ssim += batch_ssim / (output_np.shape[0] * output_np.shape[1])\n    \n    avg_mse = total_mse / num_batches\n    avg_ssim = total_ssim / num_batches\n    print(f\"Validation MSE: {avg_mse:.4f}, SSIM: {avg_ssim:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = PredRNN(input_dim=1, hidden_dim=32, num_layers=3).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# Training\ntrain_model(\n    model,\n    train_loader,\n    optimizer,\n    criterion,\n    device,\n    num_epochs=5,\n    target_length=10  # Set your desired target_length\n)\n\n# Evaluation\nevaluate_model(\n    model,\n    val_loader,\n    device,\n    target_length=10  # Match the target_length used during training\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Transformer-based","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom skimage.metrics import structural_similarity as ssim\n\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size=64, patch_size=8, in_chans=1, embed_dim=128):\n        super(PatchEmbedding, self).__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        self.embed_dim = embed_dim\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        # x shape: [B, C, H, W]\n        x = self.proj(x)  # [B, embed_dim, H/patch_size, W/patch_size]\n        x = x.flatten(2)  # [B, embed_dim, num_patches]\n        x = x.transpose(1, 2)  # [B, num_patches, embed_dim]\n        return x\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, num_patches, embed_dim):\n        super(PositionalEncoding, self).__init__()\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n\n    def forward(self, x):\n        x = x + self.pos_embed\n        return x\n\nclass TransformerModel(nn.Module):\n    def __init__(self, img_size=64, patch_size=8, in_chans=1, embed_dim=128, num_layers=6, num_heads=8, mlp_dim=256, dropout=0.1):\n        super(TransformerModel, self).__init__()\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n        num_patches = (img_size // patch_size) ** 2\n        self.pos_embed = PositionalEncoding(num_patches, embed_dim)\n\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n        self.decoder = nn.Linear(embed_dim, patch_size * patch_size * in_chans)\n\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n        self.embed_dim = embed_dim\n        self.in_chans = in_chans\n\n    def forward(self, x):\n        # x shape: [B, T, C, H, W]\n        B, T, C, H, W = x.size()\n        x = x.view(B * T, C, H, W)  # [B*T, C, H, W]\n        x = self.patch_embed(x)  # [B*T, num_patches, embed_dim]\n        x = self.pos_embed(x)\n\n        x = self.transformer_encoder(x)  # [B*T, num_patches, embed_dim]\n        x = self.decoder(x)  # [B*T, num_patches, patch_size * patch_size * in_chans]\n        x = x.transpose(1, 2)  # [B*T, patch_size * patch_size * in_chans, num_patches]\n\n        # Reshape to reconstruct the images\n        x = x.view(B*T, self.in_chans, self.patch_size, self.patch_size, self.num_patches)\n        x = x.permute(0, 1, 4, 2, 3)  # [B*T, C, num_patches, patch_size, patch_size]\n        x = x.contiguous().view(B*T, C, self.img_size, self.img_size)  # [B*T, C, H, W]\n\n        x = x.view(B, T, C, H, W)  # [B, T, C, H, W]\n        x = torch.sigmoid(x)  # Ensure output is between 0 and 1\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Instantiate the Transformer model\nmodel = TransformerModel(\n    img_size=64,\n    patch_size=32,\n    in_chans=1,\n    embed_dim=128,\n    num_layers=6,\n    num_heads=8,\n    mlp_dim=256,\n    dropout=0.1\n)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Define loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training parameters\nnum_epochs = 10\n\n# Initialize lists to store losses and metrics\ntrain_losses = []\nval_losses = []\nval_mse_scores = []\nval_ssim_scores = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        # Concatenate input and target sequences\n        input_seq = torch.cat([X_batch, y_batch], dim=1)  # [B, T_input + T_target, C, H, W]\n        input_seq = input_seq.to(device)\n        y_batch = y_batch.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_seq)\n        outputs = outputs[:, X_batch.size(1):]  # Get only the predicted frames\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * X_batch.size(0)\n    \n    train_loss /= len(train_loader.dataset)\n    train_losses.append(train_loss)\n    \n    # Validation phase\n    model.eval()\n    val_loss = 0.0\n    total_mse = 0.0\n    total_ssim = 0.0\n    num_samples = 0\n    \n    with torch.no_grad():\n        for X_batch, y_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n            input_seq = torch.cat([X_batch, y_batch], dim=1)\n            input_seq = input_seq.to(device)\n            y_batch = y_batch.to(device)\n            outputs = model(input_seq)\n            outputs = outputs[:, X_batch.size(1):]\n            loss = criterion(outputs, y_batch)\n            val_loss += loss.item() * X_batch.size(0)\n            \n            # Compute MSE and SSIM per frame\n            batch_size = X_batch.size(0)\n            num_samples += batch_size\n            outputs_np = outputs.cpu().numpy()\n            y_batch_np = y_batch.cpu().numpy()\n            \n            for i in range(batch_size):\n                mse_per_video = []\n                ssim_per_video = []\n                for t in range(outputs_np.shape[1]):\n                    # Predicted and ground truth frames\n                    pred_frame = outputs_np[i, t, 0]\n                    true_frame = y_batch_np[i, t, 0]\n                    \n                    # Compute MSE\n                    mse_frame = np.mean((pred_frame - true_frame) ** 2)\n                    mse_per_video.append(mse_frame)\n                      \n                    # Compute SSIM\n                    ssim_frame = ssim(true_frame, pred_frame, data_range=1.0)\n                    ssim_per_video.append(ssim_frame)\n                \n                # Average over frames in the sequence\n                total_mse += np.mean(mse_per_video)\n                total_ssim += np.mean(ssim_per_video)\n    \n    val_loss /= len(val_loader.dataset)\n    val_losses.append(val_loss)\n    \n    # Average MSE and SSIM over the validation set\n    avg_mse = total_mse / num_samples \n    avg_ssim = total_ssim / num_samples \n    val_mse_scores.append(avg_mse)   \n    val_ssim_scores.append(avg_ssim)    \n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}, MSE: {avg_mse:.6f}, SSIM: {avg_ssim:.6f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Video Generation","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model, dataset, sample_index=0):\n    \"\"\"\n    Generate predicted frames using the model for a sample from the dataset.\n\n    Args:\n        model: Trained Transformer model.\n        dataset: The dataset to sample from (e.g., val_dataset).\n        sample_index: Index of the sample in the dataset to use.\n\n    Returns:\n        input_frames: Numpy array of input frames.\n        pred_frames: Numpy array of predicted frames.\n    \"\"\"\n    model.eval()\n    device = next(model.parameters()).device\n\n    # Get a sample from the dataset\n    input_seq, target_seq = dataset[sample_index]\n    input_seq = input_seq.unsqueeze(0)  # Add batch dimension\n    target_seq = target_seq.unsqueeze(0)  # Add batch dimension\n\n    # Concatenate input and target sequences\n    input_and_target = torch.cat([input_seq, target_seq], dim=1).to(device)\n\n    with torch.no_grad():\n        # Generate predictions\n        output_seq = model(input_and_target)\n        pred_seq = output_seq[:, input_seq.size(1):]  # Get the predicted frames\n        pred_seq = pred_seq.squeeze(0).cpu()  # Remove batch dimension and move to CPU\n\n    # Convert tensors to numpy arrays and rescale to [0, 255]\n    input_frames = input_seq.squeeze(0).numpy()  # Shape: [input_length, channels, H, W]\n    pred_frames = pred_seq.numpy()  # Shape: [target_length, channels, H, W]\n\n    # Rescale pixel values from [0, 1] to [0, 255]\n    input_frames = (input_frames * 255).astype(np.uint8)\n    pred_frames = (pred_frames * 255).astype(np.uint8)\n\n    # Remove channel dimension if channels == 1\n    if input_frames.shape[1] == 1:\n        input_frames = input_frames.squeeze(1)  # Shape: [input_length, H, W]\n        pred_frames = pred_frames.squeeze(1)    # Shape: [target_length, H, W]\n\n    return input_frames, pred_frames","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate predictions\ninput_frames, pred_frames = generate_predictions(model, val_dataset, sample_index=1)\n\n# Visualize the frames (optional)\nvisualize_combined_frames(input_frames, pred_frames)\n\n# Combine input and predicted frames\nall_frames = np.concatenate((input_frames, pred_frames), axis=0)  # Shape: [total_frames, H, W]\n\n# Save the video using OpenCV\nsave_frames_as_video(all_frames, save_path='/kaggle/working/transformer_generated_video.mp4', fps=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# User Interface","metadata":{}},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport cv2\nimport os\n\n# Function to extract frames\ndef extract_frames(video_path, output_folder=\"/kaggle/working/temp_frames\", frame_size=(64, 64)):\n    \"\"\"\n    Extract frames from a video, resize to 64x64, and save to the specified folder.\n\n    Parameters:\n        video_path (str): Path to the video file.\n        output_folder (str): Directory to save extracted frames.\n        frame_size (tuple): Target frame size (width, height).\n\n    Returns:\n        list: List of file paths to the extracted frames.\n    \"\"\"\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n        print(f\"Directory created: {output_folder}\")\n    else:\n        print(f\"Directory already exists: {output_folder}\")\n\n    video_capture = cv2.VideoCapture(video_path)\n    frame_count = 0\n    success = True\n    extracted_frames = []\n\n    while success:\n        success, frame = video_capture.read()\n        if success:\n            # Resize and convert to grayscale\n            frame = cv2.resize(frame, frame_size, interpolation=cv2.INTER_AREA)\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n            # Save frame as image\n            frame_filename = f\"frame_{frame_count:05d}.jpg\"\n            frame_filepath = os.path.join(output_folder, frame_filename)\n            cv2.imwrite(frame_filepath, frame)\n\n            extracted_frames.append(frame_filepath)\n            frame_count += 1\n\n    video_capture.release()\n    print(f\"Extracted {len(extracted_frames)} frames.\")\n    return extracted_frames\n\n\n# Function to process video input\ndef process_video(uploaded_video, selected_sample):\n    \"\"\"\n    Process the uploaded or selected video and extract frames.\n\n    Parameters:\n        uploaded_video (Gradio file): Uploaded video by user.\n        selected_sample (str): Path to a selected video from test dataset.\n\n    Returns:\n        list: List of paths to extracted frames.\n    \"\"\"\n    if uploaded_video is not None:\n        # Save uploaded video to /kaggle/working/temp_videos\n        temp_dir = \"/kaggle/working/temp_videos\"\n        os.makedirs(temp_dir, exist_ok=True)\n        video_path = os.path.join(temp_dir, uploaded_video.name)\n        with open(video_path, \"wb\") as f:\n            f.write(uploaded_video.read())\n        print(f\"Uploaded video saved at: {video_path}\")\n    elif selected_sample is not None:\n        # Use selected sample video path\n        video_path = selected_sample\n        print(f\"Selected sample video path: {video_path}\")\n    else:\n        print(\"No video input provided.\")\n        return []\n\n    # Extract frames\n    extracted_frames = extract_frames(video_path, output_folder=\"/kaggle/working/temp_frames\")\n    return extracted_frames\n\n\n# Video paths for Dropdown (example paths)\nvideo_paths = [\n    \"/kaggle/input/ucf101-action-recognition/test/JumpingJack/k_JumpingJack_g12_c04.avi\",\n    \"/kaggle/input/ucf101-action-recognition/test/Walking/k_Walking_g12_c04.avi\",\n    \"/kaggle/input/ucf101-action-recognition/test/Biking/k_Biking_g12_c04.avi\",\n]\n\n# Gradio Interface\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Video Processing Interface\")\n    gr.Markdown(\"Upload a video or select a video from the test folder to process and extract frames.\")\n\n    with gr.Row():\n        with gr.Column():\n            # Upload option\n            video_input = gr.File(label=\"Upload Video\", file_types=[\".mp4\", \".avi\", \".mkv\"])\n            # Dropdown selection\n            sample_input = gr.Dropdown(\n                choices=video_paths,\n                label=\"Or Select a Video from Test Folder\",\n                interactive=True\n            )\n            run_button = gr.Button(\"Run Models\")\n        with gr.Column():\n            with gr.Tab(\"Model 1\"):\n                gr.Markdown(\"### Model 1 Results\")\n                frames_output1 = gr.Gallery(label=\"Extracted Frames\", columns=5, height=\"auto\")  # For testing\n                video_output1 = gr.Video(label=\"Final Video\")  # Placeholder\n                time_output1 = gr.Textbox(label=\"Inference Time\")  # Placeholder\n            with gr.Tab(\"Model 2\"):\n                gr.Markdown(\"### Model 2 Results\")\n                frames_output2 = gr.Gallery(label=\"Generated Frames\", columns=2, height=\"auto\")\n                video_output2 = gr.Video(label=\"Final Video\")\n                time_output2 = gr.Textbox(label=\"Inference Time\")\n            with gr.Tab(\"Model 3\"):\n                gr.Markdown(\"### Model 3 Results\")\n                frames_output3 = gr.Gallery(label=\"Generated Frames\", columns=2, height=\"auto\")\n                video_output3 = gr.Video(label=\"Final Video\")\n                time_output3 = gr.Textbox(label=\"Inference Time\")\n\n    # Handle button click\n    def handle_run(uploaded_video, selected_sample):\n        frames = process_video(uploaded_video, selected_sample)\n        return frames, None, \"0.0 seconds\", [], None, None, [], None, None\n\n    run_button.click(\n        handle_run,\n        inputs=[video_input, sample_input],\n        outputs=[\n            frames_output1, video_output1, time_output1,\n            frames_output2, video_output2, time_output2,\n            frames_output3, video_output3, time_output3\n        ]\n    )\n\n    gr.Markdown(\"© 2024 Your Application\")\n\n# Launch the application\ndemo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}